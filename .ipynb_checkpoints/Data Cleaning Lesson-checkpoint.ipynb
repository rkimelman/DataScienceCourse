{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling Course\n",
    "## Lesson 2, Dealing with Dirty Data\n",
    "\n",
    "In this assignment, you'll be implementing techniques for cleaning up \"dirty data.\"\n",
    "\n",
    "Dirty data consists of inconsistencies in the data such as missing values that will lead to certain functions crashing or misleading data, thus why it is important we use techniques for making the data more readable and understandable before doing certain analyses. In short, any data that takes away from the \"data integrity\" of a dataset is considered dirty data.\n",
    "\n",
    "There are other examples, for instance in the business world, such as data collected using the wrong method, from the wrong population, and inaccurate data, for example a small mistake in a customer address. We will not look at those because there is no way for us to identify and pinpoint these errors. Most significant dirty data comes from these errors, but the cleaning that we do is still an integral part, as it is said to take up 80% of a data scientist's time.\n",
    "\n",
    "## Getting started\n",
    "There are some common issues with datasets that we will be looking at. These include (but are not limited to) missing values, units, duplicate data, having values that don't fall within a specified range, redundant data, and data with different columns for the same data points but in different units.\n",
    "\n",
    "If you think of or come across any other examples not covered in this lesson, please contact either a TA, the course instructor, or your graduate school/PhD advisor and we will discuss what strategies might be implemented for such data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the ncaa dataset in your dataset folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read your data into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling dirty data can be tricky. When evaluating how to handle missing or na values, one can consider the following:\n",
    "\n",
    "    - Remove entire row if any value is missing\n",
    "    - Remove entire row if multiple values are missing\n",
    "    - Fill in the n/a or missing value with the average or median of the data set\n",
    "    \n",
    "These are just a few solutions to cleaning missing values, however each data set's solution may look different. When cleaning data it is important to document how you handled cleaning the data, so that others can reproduce your work and you can remove honest about any data modifications.\n",
    "\n",
    "Also when cleaning data make sure that you don't add bias or change the dataset beyond recognition. Take for example a data set where one of the fields is what time people eat dinner. Through mindlessly removing all rows that have eranious values for columns, you may neglect the fact that elderly people tend to have a higher tendancy to produce eranious data. Now your data-set may be transformed to the point where you have removed almost all rows that contain people eating the early-bird special."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For practice try the following: remove the entire row from our dataframe if any value in the row is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try dropping the entire row if all values in the row are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-standard types of missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On top of missing values, your data set can sometimes include textual missing values. These can include, but are far from limited to: \n",
    " - n/a\n",
    " - na\n",
    " - '--'\n",
    " \n",
    "List some of the different types of textual missing values that exist in our NCAA dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip:** Try using some of these methods and figuring out exactly what they do to get a better picture of your dataset.\n",
    "\n",
    " - .info()\n",
    " - .isnull()\n",
    " - .isnull().any()\n",
    " - .isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-read in the data and include these missing values as your 'na_values':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use .fillna() to replace missing values. It is common practice to either fill the missing values with 0, hot-deck, cold-deck, mean substitution, regression, and multiple imputation. Check out this [wikipedia article](https://en.wikipedia.org/wiki/Imputation_(statistics)) on it. Which is best? What do you do if its a string?\n",
    "\n",
    "Please choose of the following replacement options and put them to use for the NCAA dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hot-deck:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_list = data['column'].tolist()\n",
    "random_point = random.choice(data_to_list)\n",
    "if(random_point != 0 and random_point.isNaN() and random_point != na_values):\n",
    "    data['column'] = data['column'].fillna(random_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cold-deck - select a random point from another dataset (similar to above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean substitution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median substitution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression (Perform linear or logistic regression on another column and use that to predict what the value in this column would be):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple imputation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.imputation.mice as mice\n",
    "from fancyimpute import MICE\n",
    "data_cols = list(data)\n",
    "data = pd.DataFrame(MICE(verbose=False).complete(data))\n",
    "data.columns = data_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN can also be used, but it becomes inefficient with large datasets, so we will ignore it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicate data occurs when there are two rows that have all of the same column values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use .duplicated() on dataframe to return boolean series with true for each duplicated row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now remove duplicated data:\n",
    "\n",
    "**Tip:** There is a library function for this, try finding it online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Values that don't fall within a specified range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide if there are columns that should have only a certain range of values and apply any of the above methods for \"replacing missing values\" for any values that don't fall within that range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redundant Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redundant data occurs when there is a data point that is repeated that could have been deduced from another data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"img/redundant_data.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find unique values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slice the columns that are redundant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data with different columns for same variable, but in different units:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slice the column with the non-standard units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
